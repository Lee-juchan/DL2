{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smartdesignlab/SDL_teaching/blob/main/KAIST_SDL_1_Generative_AI_DDIM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r0UFpmzp5hB_",
      "metadata": {
        "id": "r0UFpmzp5hB_"
      },
      "source": [
        "# **1. Generative AI: Denoising Diffusion Implicit Models (DDIM)**\n",
        "\n",
        "![DDIM](https://drive.google.com/uc?id=10IVOM5qJYd9N3KTEgd7IPhnS0R9TM_tB)\n",
        "**Overview**:  \n",
        "\n",
        "1) Import libraries and set up environment.  \n",
        "2) Define parameters.  \n",
        "3) Load and preprocess data.  \n",
        "4) Initialize model and scheduler.  \n",
        "5) Run the training loop.  \n",
        "6) Visualize and analyze results.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QY-2vstJ5hCD",
      "metadata": {
        "id": "QY-2vstJ5hCD"
      },
      "source": [
        "## 1) Import Libraries and Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92430214",
      "metadata": {
        "id": "92430214"
      },
      "source": [
        "### **| Load dataset & Pre-trained model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SyzJS_ESgcRF",
      "metadata": {
        "id": "SyzJS_ESgcRF"
      },
      "source": [
        "![Dataset](https://drive.google.com/uc?id=1lgZ5KQWqZ_SFswYQktRlDXXFFBoD36d2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wk9dmd1B5hCF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142,
          "referenced_widgets": [
            "09c7e6451dbc4456bb8dafc2c61c8e75",
            "e82c5d39d8a045f8884fcbb0501fd484",
            "3050a87d0d384aa386c4238da45c86c7",
            "48537c241bf84c58b50d839636eae993",
            "24bf43fbae87406781ebf7a8758ae372",
            "6854f87d041946be86ba8d27794188bb",
            "6ef1ec26b3c240da9115ef54abe2fcdb",
            "07e8688eaa29463da698742f3d93cafe",
            "9be4ef4e65f64625b8492778338c3c7e",
            "2e7f1204e7a0429eaabcf4d2884b8b27",
            "2db76c7f29c7450cae638be80d35e8c4"
          ]
        },
        "id": "wk9dmd1B5hCF",
        "outputId": "bbe7a242-4781-43bf-8dce-76f444d439f9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "from diffusers import UNet2DModel, DDIMScheduler\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import time\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.utils import save_image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.transforms as T\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Fix random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def img_read(src, file):\n",
        "    \"\"\"Read grayscale image as a NumPy array.\"\"\"\n",
        "    img = cv.imread(os.path.join(src, file), cv.IMREAD_GRAYSCALE)\n",
        "    return img\n",
        "\n",
        "\n",
        "def show_img(images_list,\n",
        "             r=1,\n",
        "             cmap='gray',\n",
        "             img_size=(5, 5),\n",
        "             axis=\"off\",\n",
        "             colorbar=False,\n",
        "             colorbar_range=None,\n",
        "             save_path=None):\n",
        "    if r < 1:\n",
        "        r = 1\n",
        "\n",
        "    total_images = len(images_list)\n",
        "    if total_images == 0:\n",
        "        print(\"No images to display.\")\n",
        "        return\n",
        "\n",
        "    cols = (total_images + r - 1) // r\n",
        "    fig, axs = plt.subplots(r, cols, figsize=(cols * img_size[0], r * img_size[1]))\n",
        "\n",
        "    if r == 1:\n",
        "        axs = axs.reshape(1, -1)\n",
        "\n",
        "    for idx, item in enumerate(images_list):\n",
        "        ax = axs[0, idx] if r == 1 else axs[idx // cols, idx % cols]\n",
        "        im = None\n",
        "        if isinstance(item, Image.Image):\n",
        "            if item.mode in ['L', '1']:  # Grayscale images\n",
        "                im = ax.imshow(item, cmap=cmap)\n",
        "            else:  # Color images\n",
        "                im = ax.imshow(item)\n",
        "        elif isinstance(item, np.ndarray):\n",
        "            if item.ndim == 2:  # 2D array, grayscale image\n",
        "                im = ax.imshow(item, cmap=cmap, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 3:  # 3D array, color image\n",
        "                im = ax.imshow(item, cmap=cmap if item.shape[-1] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 4 and item.shape[0] == 1:  # 4D array with batch dimension of 1\n",
        "                im = ax.imshow(item[0], cmap=cmap if item.shape[1] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported numpy array shape: {item.shape}.\")\n",
        "        elif torch.is_tensor(item):\n",
        "            item = item.detach().cpu().numpy()\n",
        "            if item.ndim == 2:  # 2D tensor, grayscale image\n",
        "                im = ax.imshow(item, cmap=cmap, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 3:  # 3D tensor, color image\n",
        "                im = ax.imshow(item.transpose(1, 2, 0), cmap=cmap if item.shape[0] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            elif item.ndim == 4 and item.shape[0] == 1:  # 4D tensor with batch dimension of 1\n",
        "                im = ax.imshow(item[0].transpose(1, 2, 0), cmap=cmap if item.shape[1] == 1 else None, vmin=colorbar_range[0] if colorbar_range else None, vmax=colorbar_range[1] if colorbar_range else None)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported torch tensor shape: {item.shape}.\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported image type. Supported types: PIL Image, numpy array, torch tensor.\")\n",
        "\n",
        "        if colorbar and im is not None:\n",
        "            divider = make_axes_locatable(ax)\n",
        "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "            fig.colorbar(im, cax=cax)\n",
        "\n",
        "        ax.axis(axis)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def show_img_subplot(images_list, r=6, c=6, cmap='gray', img_size=(2, 2), axis=\"off\", colorbar=False, colorbar_range=None, save_path=None):\n",
        "    \"\"\"Display multiple images in a 6x6 grid.\"\"\"\n",
        "    total_images = len(images_list)\n",
        "    fig, axs = plt.subplots(r, c, figsize=(c * img_size[0] * 0.6, r * img_size[1] * 0.6))\n",
        "\n",
        "    for idx, item in enumerate(images_list[:r * c]):\n",
        "        ax = axs[idx // c, idx % c]\n",
        "        if torch.is_tensor(item):\n",
        "            item = item.detach().cpu().numpy()\n",
        "\n",
        "        if item.ndim == 2:\n",
        "            im = ax.imshow(item, cmap=cmap,\n",
        "                           vmin=colorbar_range[0] if colorbar_range else None,\n",
        "                           vmax=colorbar_range[1] if colorbar_range else None)\n",
        "        elif item.ndim == 3:\n",
        "            if item.shape[0] < 10:\n",
        "                item = np.transpose(item, (1, 2, 0))\n",
        "            im = ax.imshow(item, cmap=cmap,\n",
        "                           vmin=colorbar_range[0] if colorbar_range else None,\n",
        "                           vmax=colorbar_range[1] if colorbar_range else None)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported shape: {item.shape}\")\n",
        "\n",
        "        ax.axis(axis)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def get_data():\n",
        "    \"\"\"Load images and normalize.\"\"\"\n",
        "    img_files = [f for f in os.listdir(src) if f.endswith('.png')]\n",
        "    images = []\n",
        "\n",
        "    for file in img_files[:36]:  # Load only 36 images\n",
        "        index_str = os.path.splitext(file)[0]\n",
        "        try:\n",
        "            _ = int(index_str)\n",
        "        except ValueError:\n",
        "            continue\n",
        "        img = img_read(src, file)\n",
        "        img_t = T.Compose([\n",
        "            T.ToPILImage(mode='L'),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.5], std=[0.5])\n",
        "        ])(img)\n",
        "        images.append(img_t.numpy())\n",
        "\n",
        "    images = np.array(images)\n",
        "    return images\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fckQc3km5hCG",
      "metadata": {
        "id": "fckQc3km5hCG"
      },
      "source": [
        "## 2) Define Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qryncKkK5hCH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qryncKkK5hCH",
        "outputId": "8175f3be-0d68-4006-c271-0fb479ff417b"
      },
      "outputs": [],
      "source": [
        "batch_size = 36 # Number of images processed per iteration\n",
        "eta = 0.0  # DDIM parameter: control sampling stochasticity\n",
        "seed = 0\n",
        "set_seed(seed)\n",
        "\n",
        "src = './dataset/data_56x56_36/'\n",
        "\n",
        "img = Image.open('./dataset/data_56x56_36/1.png')\n",
        "\n",
        "print(\"Batch size:\", batch_size)\n",
        "print(\"Image size:\", img.size)\n",
        "print(\"Seed:\", seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fU6bZWrz5hCH",
      "metadata": {
        "id": "fU6bZWrz5hCH"
      },
      "source": [
        "## 3) Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LTQxmk2_5hCH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "LTQxmk2_5hCH",
        "outputId": "401c9237-9870-4225-ffab-35b8cf71442e"
      },
      "outputs": [],
      "source": [
        "# Load and convert to tensors\n",
        "train_images = torch.tensor(get_data(), dtype=torch.float32)\n",
        "\n",
        "print(\"Train images shape:\", train_images.shape)\n",
        "\n",
        "# Visualize loaded images\n",
        "show_img_subplot(train_images[:36], r=6, c=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "viMI9xfm5hCH",
      "metadata": {
        "id": "viMI9xfm5hCH"
      },
      "source": [
        "## 4) Initialize Model and Scheduler\n",
        "We create a `UNet2DModel` to predict noise and a `DDIMScheduler` to handle noise scheduling. We also set up our DataLoaders, optimizer, and LR scheduler."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2Fy1jPlWkmZw",
      "metadata": {
        "id": "2Fy1jPlWkmZw"
      },
      "source": [
        "### **| UNet architecture**\n",
        "![UNet](https://drive.google.com/uc?id=1bSaQgiUU439P38I6w_r60Ia37vslZQLZ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sjGtUjqM5hCI",
      "metadata": {
        "id": "sjGtUjqM5hCI"
      },
      "outputs": [],
      "source": [
        "# UNet model\n",
        "unet = UNet2DModel(\n",
        "    sample_size=56,\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    layers_per_block=1,\n",
        "    block_out_channels=[64, 128, 256, 512],\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99d8fa45",
      "metadata": {
        "id": "99d8fa45"
      },
      "source": [
        "It receives `noisy_images` and `timestep` as inputs and outputs the predicted noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1b54f79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "c1b54f79",
        "outputId": "f62d41de-f76c-4f0d-d0d6-df330e099433"
      },
      "outputs": [],
      "source": [
        "noisy_images = torch.randn((4, 1, 56, 56)).to(device)\n",
        "timestep = torch.tensor([10]).to(device)\n",
        "with torch.no_grad():\n",
        "    pred_noises = unet(noisy_images, timestep).sample\n",
        "print(pred_noises.shape)\n",
        "show_img(pred_noises)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30cf0b39",
      "metadata": {
        "id": "30cf0b39"
      },
      "source": [
        "### **| Image Noising Process (*Forward Diffusion Process*)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52bcc968",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52bcc968",
        "outputId": "caab932b-c58b-4dfb-a385-284dd74e22dc"
      },
      "outputs": [],
      "source": [
        "noise_scheduler = DDIMScheduler(num_train_timesteps=600)\n",
        "noise_scheduler.set_timesteps(5)\n",
        "print('timesteps : ', len(noise_scheduler.timesteps), noise_scheduler.timesteps) # check the timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65d9054",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d65d9054",
        "outputId": "a513e28e-91e7-460e-f17e-18280b5b2bd4"
      },
      "outputs": [],
      "source": [
        "images = train_images[:4].to(device)\n",
        "for timestep in reversed(noise_scheduler.timesteps):\n",
        "    print(timestep)\n",
        "\n",
        "    noises = torch.randn(images.shape).to(device)\n",
        "    noisy_images = noise_scheduler.add_noise(images, noises, timestep)\n",
        "    show_img(noisy_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08512684",
      "metadata": {
        "id": "08512684"
      },
      "source": [
        "### **| Image Denoising Process (*Reverse Diffusion Process*)**\n",
        "\n",
        "Use UNet and the DDIM Noise Scheduler to implement a progressive restoration of noisy images. Guide codes are available at [Hugging Face DDIM Pipeline] (https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/ddim/pipeline_ddim.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20979e8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "20979e8e",
        "outputId": "588ce175-3a53-495a-cda1-b2ca0550411a"
      },
      "outputs": [],
      "source": [
        "images =  torch.randn((4, 1, 56, 56)).to(device) # set up initial noise\n",
        "noise_scheduler.set_timesteps(4)\n",
        "for timestep in noise_scheduler.timesteps: # Repeat as many times as timesteps\n",
        "\n",
        "    with torch.no_grad(): # deactivate gradient calculation\n",
        "        pred_noises = unet(images, timestep).sample # UNet pipeline\n",
        "        images  = noise_scheduler.step(pred_noises, timestep, images).prev_sample # denoising step\n",
        "\n",
        "images = images.cpu().detach().numpy()\n",
        "print(images.shape)\n",
        "show_img(images[:,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfe49b93",
      "metadata": {
        "id": "dfe49b93"
      },
      "source": [
        "### **| Defining the Diffusion Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66776bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c66776bc",
        "outputId": "191367db-84ba-4fac-9905-d6b52b41bdf2"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "num_train_timesteps = 600\n",
        "num_epochs = 1000\n",
        "lr_warmup_steps = 100\n",
        "learning_rate = 1e-4\n",
        "max_norm = 1  # gradient clipping threshold\n",
        "\n",
        "# DDIM scheduler\n",
        "noise_scheduler = DDIMScheduler(num_train_timesteps=num_train_timesteps)\n",
        "\n",
        "# Create dataset/loader\n",
        "train_dataset = TensorDataset(train_images)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Optimizer and LR scheduler\n",
        "optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataset) * num_epochs),\n",
        ")\n",
        "\n",
        "output_dir = './'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(\"Model, scheduler, optimizer, and DataLoader are set.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DArFBMvF5hCJ",
      "metadata": {
        "id": "DArFBMvF5hCJ"
      },
      "source": [
        "## 5) Training Loop\n",
        "1. Add random noise to images based on a random timestep.\n",
        "2. Predict the noise using UNet.\n",
        "3. Compute loss and backprop.\n",
        "4. Validate with a test set.\n",
        "5. Save best models.\n",
        "6. Periodically sample and save generated images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LHwL_Y_g5hCJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHwL_Y_g5hCJ",
        "outputId": "823fb7b6-cea1-4f09-ac5a-f23416f3e3f0"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "best_train_loss = float(\"inf\")\n",
        "loss_history = {\"train_loss\": []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start = time.time()\n",
        "    unet.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Training\n",
        "    for step, imgs in enumerate(train_loader):\n",
        "        imgs = imgs[0].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 1) Add noise\n",
        "        noise = torch.randn_like(imgs)\n",
        "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,\n",
        "                                  (imgs.shape[0],), device=device).long()\n",
        "        noisy_images = noise_scheduler.add_noise(imgs, noise, timesteps)\n",
        "\n",
        "        # 2) UNet forward\n",
        "        noise_pred = unet(noisy_images, timesteps).sample\n",
        "\n",
        "        # 3) Compute loss\n",
        "        loss = F.mse_loss(noise_pred, noise) # UNet-predicted noise, Added noise\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(unet.parameters(), max_norm)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss_train = total_loss / len(train_loader)\n",
        "    loss_history[\"train_loss\"].append(average_loss_train)\n",
        "\n",
        "    epoch_end = time.time()\n",
        "    epoch_duration = epoch_end - epoch_start\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {average_loss_train:.6f} | Time: {epoch_duration:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m9rms6tn5hCJ",
      "metadata": {
        "id": "m9rms6tn5hCJ"
      },
      "source": [
        "## 6) Image generation (inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DE2otLrN5hCK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "DE2otLrN5hCK",
        "outputId": "f5d2676d-3517-4276-9b35-dfe960afdfb9"
      },
      "outputs": [],
      "source": [
        "# Sigmoid filter for post-processing\n",
        "def sigmoid_filter(img, threshold=0.0, alpha=10.0):\n",
        "    return torch.sigmoid(alpha * (img - threshold))\n",
        "\n",
        "unet.load_state_dict(torch.load(\"Generator_DDIM.pth\", map_location=device))\n",
        "unet.eval()\n",
        "\n",
        "num_inference_steps = 50\n",
        "noise_scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "images = torch.randn((10, 1, 56, 56)).to(device)\n",
        "\n",
        "for timestep in noise_scheduler.timesteps:\n",
        "    with torch.no_grad():\n",
        "        pred_noises = unet(images, timestep).sample\n",
        "    images = noise_scheduler.step(pred_noises, timestep.long(), images).prev_sample\n",
        "\n",
        "images = sigmoid_filter(images, threshold=0.0, alpha=10.0)\n",
        "show_img(images.cpu().detach().numpy()[:, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db19a9f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "db19a9f0",
        "outputId": "febe2df8-b4f0-4aaa-f364-93afaef32c05"
      },
      "outputs": [],
      "source": [
        "num_images = 100  # Number of images to generate\n",
        "\n",
        "# Output directory\n",
        "output_dir = \"./DDIM_Generated_images\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Initialize DDIM Scheduler\n",
        "noise_scheduler.set_timesteps(num_inference_steps, eta)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "generator = torch.manual_seed(0)\n",
        "\n",
        "# Generate images\n",
        "print(\"Generating images...\")\n",
        "generated_images = []\n",
        "for i in range(1, num_images + 1):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Generating image {i}/{num_images}...\")\n",
        "\n",
        "    # Generate random noise\n",
        "    images = torch.randn((1, 1, 56, 56), generator=generator).to(device)\n",
        "\n",
        "    # DDIM Inference (denoising step)\n",
        "    for timestep in noise_scheduler.timesteps:\n",
        "        with torch.no_grad():\n",
        "            pred_noises = unet(images, timestep).sample\n",
        "        images = noise_scheduler.step(pred_noises, timestep.long(), images).prev_sample\n",
        "\n",
        "    # Apply sigmoid filter\n",
        "    images = sigmoid_filter(images, threshold=0.0, alpha=10.0)\n",
        "\n",
        "    # Save generated image\n",
        "    image_path = os.path.join(output_dir, f\"inference_{i:03d}.png\")\n",
        "    save_image(images, image_path, normalize=True)\n",
        "\n",
        "    # Store for visualization\n",
        "    generated_images.append(images.cpu().detach().numpy()[0, 0])\n",
        "\n",
        "print(f\"Image generation completed. {num_images} images saved to {output_dir}\")\n",
        "\n",
        "# Visualize generated images\n",
        "def show_generated_images(images, rows=5, cols=5):\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(10, 10))\n",
        "    for i, ax in enumerate(axs.flatten()):\n",
        "        if i >= len(images):\n",
        "            break\n",
        "        ax.imshow(images[i], cmap='gray')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display the first 25 generated images\n",
        "show_generated_images(generated_images, rows=5, cols=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e74897",
      "metadata": {
        "id": "c8e74897"
      },
      "source": [
        "## 7) Data labeling\n",
        "![FEA](https://drive.google.com/uc?id=1aGEcPzFiDNKJJZVfqOltpFffu8POBj3G)\n",
        "\n",
        "(Andreassen, E., Clausen, A., Schevenels, M., Lazarov, B. S., & Sigmund, O. (2011). Efficient topology optimization in MATLAB using 88 lines of code. Structural and Multidisciplinary Optimization, 43, 1-16.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2ca1ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f2ca1ac",
        "outputId": "5cac40f0-6891-4c96-ee11-88e3df6e2674"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "from scipy.sparse.linalg import spsolve\n",
        "from PIL import Image\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "# 1) Element stiffness matrix (Q4)\n",
        "\n",
        "def Q4_KE(nu=0.3):\n",
        "    \"\"\"Compute element stiffness matrix for Q4 elements.\"\"\"\n",
        "    a = np.array([12, 3, -6, 3, 0, -6, -3, -3], dtype=float)\n",
        "    b = np.array([-4, 3, -2, -9, 4, 2, -3, 9], dtype=float)\n",
        "    k = (a + nu * b) / (24.0 * (1.0 - nu ** 2))\n",
        "\n",
        "    i1 = np.array([\n",
        "        [1, 2, 3, 8],\n",
        "        [2, 1, 4, 5],\n",
        "        [3, 4, 1, 7],\n",
        "        [8, 5, 7, 1]\n",
        "    ]) - 1\n",
        "    i2 = np.array([\n",
        "        [6, 7, 5, 4],\n",
        "        [7, 6, 8, 3],\n",
        "        [5, 8, 6, 2],\n",
        "        [4, 3, 2, 6]\n",
        "    ]) - 1\n",
        "\n",
        "    cIndex = np.block([\n",
        "        [i1, i2],\n",
        "        [i2, i1]\n",
        "    ])\n",
        "    KE = np.zeros((8, 8), dtype=float)\n",
        "    for rr in range(8):\n",
        "        for cc in range(8):\n",
        "            KE[rr, cc] = k[cIndex[rr, cc]]\n",
        "    return KE\n",
        "\n",
        "\n",
        "# 2) Generate edofMat for nelx x nely Q4 elements\n",
        "\n",
        "def generate_edofMat(nelx, nely):\n",
        "    \"\"\"\n",
        "    Each element has 4 nodes (8 DOFs).\n",
        "    Node indices mapped in row-major order.\n",
        "    \"\"\"\n",
        "    nn = (nelx + 1) * (nely + 1)\n",
        "    nel = nelx * nely\n",
        "    edofMat = np.zeros((nel, 8), dtype=int)\n",
        "    nodeids = np.arange(nn).reshape((nely + 1, nelx + 1))\n",
        "\n",
        "    idx = 0\n",
        "    for iy in range(nely):\n",
        "        for ix in range(nelx):\n",
        "            n1 = nodeids[iy, ix]\n",
        "            n2 = nodeids[iy, ix + 1]\n",
        "            n3 = nodeids[iy + 1, ix + 1]\n",
        "            n4 = nodeids[iy + 1, ix]\n",
        "            edofMat[idx, :] = [\n",
        "                2 * n1, 2 * n1 + 1,\n",
        "                2 * n2, 2 * n2 + 1,\n",
        "                2 * n3, 2 * n3 + 1,\n",
        "                2 * n4, 2 * n4 + 1\n",
        "            ]\n",
        "            idx += 1\n",
        "    return edofMat\n",
        "\n",
        "\n",
        "# 3) Assign E values from a 56x56 image\n",
        "\n",
        "def assign_E_from_image(img_path, nelx, nely, threshold=0.5):\n",
        "    \"\"\"\n",
        "    If pixel <= threshold, E=1.0. Otherwise, E=1e-8.\n",
        "    Flip y-axis so that the bottom of the image corresponds to ey=0.\n",
        "    \"\"\"\n",
        "    pil_img = Image.open(img_path).convert('L')\n",
        "    arr = np.array(pil_img, dtype=float) / 255.0\n",
        "    h, w = arr.shape\n",
        "    if (w != nelx) or (h != nely):\n",
        "        raise ValueError(f\"Image size ({w}x{h}) != ({nelx}x{nely}).\")\n",
        "\n",
        "    Evals = np.zeros(nelx * nely, dtype=float)\n",
        "    idx = 0\n",
        "    for ey in range(nely):\n",
        "        for ex in range(nelx):\n",
        "            pixel_val = arr[nely - 1 - ey, ex]\n",
        "            Evals[idx] = 1.0 if pixel_val <= threshold else 1e-8\n",
        "            idx += 1\n",
        "    return Evals\n",
        "\n",
        "\n",
        "# 4) Assemble global stiffness matrix\n",
        "\n",
        "def assemble_K(edofMat, Evec, KE):\n",
        "    \"\"\"\n",
        "    K matrix built from each element's stiffness KE * E.\n",
        "    \"\"\"\n",
        "    nel = edofMat.shape[0]\n",
        "    ndof = edofMat.max() + 1\n",
        "\n",
        "    iK = np.kron(edofMat, np.ones((8, 1))).flatten()\n",
        "    jK = np.kron(edofMat, np.ones((1, 8))).flatten()\n",
        "\n",
        "    sK = np.zeros(nel * 64, dtype=float)\n",
        "    for e in range(nel):\n",
        "        eFactor = Evec[e]\n",
        "        sK[e * 64:(e + 1) * 64] = (KE * eFactor).ravel()\n",
        "\n",
        "    K = coo_matrix((sK, (iK, jK)), shape=(ndof, ndof)).tocsc()\n",
        "    return K\n",
        "\n",
        "\n",
        "# 5) Boundary conditions and load vectors\n",
        "\n",
        "def build_two_load_vectors(nelx, nely):\n",
        "    \"\"\"\n",
        "    - Fixed at bottom-left node (both x,y) and bottom-right node (y).\n",
        "    - Load1: +x at top-center node.\n",
        "    - Load2: -y at bottom-center node.\n",
        "    \"\"\"\n",
        "    ndof = 2 * (nelx + 1) * (nely + 1)\n",
        "    left_bottom_node = 0\n",
        "    right_bottom_node = nelx\n",
        "    fixed = [\n",
        "        2 * left_bottom_node,\n",
        "        2 * left_bottom_node + 1,\n",
        "        2 * right_bottom_node + 1\n",
        "    ]\n",
        "    fixed = np.array(fixed, dtype=int)\n",
        "\n",
        "    top_center_node = nely * (nelx + 1) + (nelx // 2)\n",
        "    bottom_center_node = (nelx // 2)\n",
        "\n",
        "    F1 = np.zeros(ndof)\n",
        "    F1[2 * top_center_node] = 1.0\n",
        "\n",
        "    F2 = np.zeros(ndof)\n",
        "    F2[2 * bottom_center_node + 1] = -1.0\n",
        "\n",
        "    all_dofs = np.arange(ndof)\n",
        "    free_dofs = np.setdiff1d(all_dofs, fixed)\n",
        "    return free_dofs, F1, F2\n",
        "\n",
        "\n",
        "# 6) Solve linear system and compute compliance\n",
        "\n",
        "def solve_compliance(K, free_dofs, F):\n",
        "    \"\"\"\n",
        "    compliance = F^T * U\n",
        "    \"\"\"\n",
        "    ndof = K.shape[0]\n",
        "    U = np.zeros(ndof, dtype=float)\n",
        "\n",
        "    K_free = K[free_dofs, :][:, free_dofs]\n",
        "    F_free = F[free_dofs]\n",
        "    U_free = spsolve(K_free, F_free)\n",
        "    U[free_dofs] = U_free\n",
        "    return F.dot(U), U\n",
        "\n",
        "# Execute labeling using images from './DDIM_Generated_images'\n",
        "# Directory containing generated images\n",
        "image_dir = \"./DDIM_Generated_images\"\n",
        "\n",
        "# CSV file for storing compliance labels\n",
        "csv_file = os.path.join(image_dir, \"compliance_label.csv\")\n",
        "\n",
        "# Grid size\n",
        "nelx, nely = 56, 56\n",
        "\n",
        "# Precompute element stiffness matrix for Q4\n",
        "KE = Q4_KE(nu=0.3)\n",
        "\n",
        "# Build edof matrix\n",
        "edofMat = generate_edofMat(nelx, nely)\n",
        "\n",
        "# Create or initialize CSV\n",
        "if not os.path.exists(csv_file):\n",
        "    with open(csv_file, mode='w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['index', 'compliance1', 'compliance2'])\n",
        "\n",
        "# Find all PNG images\n",
        "image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n",
        "image_files.sort()\n",
        "\n",
        "print(\"Starting labeling process...\")\n",
        "for idx, image_file in enumerate(image_files, 1):\n",
        "    img_path = os.path.join(image_dir, image_file)\n",
        "\n",
        "    match = re.search(r'(\\d+)', image_file)\n",
        "    if match:\n",
        "        image_idx = int(match.group(1))\n",
        "    else:\n",
        "        print(f\"Skipping file: {image_file} (invalid format)\")\n",
        "        continue\n",
        "\n",
        "    # Assign E based on image\n",
        "    Evec = assign_E_from_image(img_path, nelx, nely, threshold=0.5)\n",
        "\n",
        "    # Assemble global stiffness matrix\n",
        "    K = assemble_K(edofMat, Evec, KE)\n",
        "\n",
        "    # Build boundary conditions and load vectors\n",
        "    free_dofs, F1, F2 = build_two_load_vectors(nelx, nely)\n",
        "\n",
        "    # Solve compliance for two load cases\n",
        "    c1, _ = solve_compliance(K, free_dofs, F1)\n",
        "    c2, _ = solve_compliance(K, free_dofs, F2)\n",
        "\n",
        "    # Remove abnormal images\n",
        "    if c1 > 300 or c2 > 300:\n",
        "        print(f\"Removing image {image_file} (compliance1={c1:.2f}, compliance2={c2:.2f})\")\n",
        "        os.remove(img_path)\n",
        "        continue\n",
        "\n",
        "    # Append results to CSV\n",
        "    with open(csv_file, mode='a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([image_idx, c1, c2])\n",
        "\n",
        "    # Print progress\n",
        "    if idx % 10 == 0:\n",
        "        print(f\"Labeled {idx}/{len(image_files)} images...\")\n",
        "\n",
        "print(f\"Labeling complete. Results saved to: {csv_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "name": "DDIM_Training_Workshop",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07e8688eaa29463da698742f3d93cafe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "09c7e6451dbc4456bb8dafc2c61c8e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e82c5d39d8a045f8884fcbb0501fd484",
              "IPY_MODEL_3050a87d0d384aa386c4238da45c86c7",
              "IPY_MODEL_48537c241bf84c58b50d839636eae993"
            ],
            "layout": "IPY_MODEL_24bf43fbae87406781ebf7a8758ae372"
          }
        },
        "24bf43fbae87406781ebf7a8758ae372": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2db76c7f29c7450cae638be80d35e8c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e7f1204e7a0429eaabcf4d2884b8b27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3050a87d0d384aa386c4238da45c86c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07e8688eaa29463da698742f3d93cafe",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9be4ef4e65f64625b8492778338c3c7e",
            "value": 0
          }
        },
        "48537c241bf84c58b50d839636eae993": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e7f1204e7a0429eaabcf4d2884b8b27",
            "placeholder": "​",
            "style": "IPY_MODEL_2db76c7f29c7450cae638be80d35e8c4",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "6854f87d041946be86ba8d27794188bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ef1ec26b3c240da9115ef54abe2fcdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9be4ef4e65f64625b8492778338c3c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e82c5d39d8a045f8884fcbb0501fd484": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6854f87d041946be86ba8d27794188bb",
            "placeholder": "​",
            "style": "IPY_MODEL_6ef1ec26b3c240da9115ef54abe2fcdb",
            "value": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
