{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1Yh-M4nmlcsqjBGTVXxQErR5LF42KzdqH\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PpOHlxlnDvn"
   },
   "source": [
    "# Convolutional Auto-Encoder(CAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxQZXje__Db7"
   },
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN_MmRu1n5C9"
   },
   "source": [
    "## 2. Import python libraries and define util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1727862010441,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "2O9iCBqiRxPw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "\n",
    "import scipy.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# function to set seed\n",
    "def set_seed(seed=42):\n",
    "    # fix random seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # CuDNN setup\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# function to read and display images\n",
    "def img_read_plot(src,file):\n",
    "    img = cv.imread(src+file,cv.COLOR_BGR2GRAY)  # convert the image to grayscale\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.xticks([])  # x axis\n",
    "    plt.yticks([])  # y axis\n",
    "    plt.show()  # display the image\n",
    "    return img\n",
    "\n",
    "# function to read images\n",
    "def img_read(src,file):\n",
    "    img = cv.imread(src+file,cv.COLOR_BGR2GRAY)  # convert the image to grayscale\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1727862011330,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "j7GAt_wAof-S"
   },
   "outputs": [],
   "source": [
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xPeogqyoTr6"
   },
   "source": [
    "## 3. Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727862011846,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "8m48zdiHR5vB",
    "outputId": "5bb60676-69e2-40b0-cfab-da592d511658"
   },
   "outputs": [],
   "source": [
    "path = '../dataset/data_3000/24.0008.jpg'\n",
    "from PIL import Image\n",
    "img = Image.open(path)\n",
    "img.show()\n",
    "img_array = np.array(img)\n",
    "print(img_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4wEiGoxJGw6"
   },
   "source": [
    "## 4. Convert images into trainable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1727862012661,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "uA7djcL7m5tu",
    "outputId": "3d7a3f5f-05bb-4ceb-ed88-52b1de7ab797"
   },
   "outputs": [],
   "source": [
    "# image size = 56 x 56, 3000 images\n",
    "\n",
    "# specify the below path to the unzipped data (ensure it ends with '/')\n",
    "# you can change the file path if needed\n",
    "src = '../dataset/data_3000/'\n",
    "\n",
    "files = os.listdir(src)  # get the list of files in the source directory\n",
    "\n",
    "X,Y = [],[]  # initialize empty lists for storing images and labels\n",
    "\n",
    "# loop through the files, read each image, and normalize it (0 to 1)\n",
    "for file in files:\n",
    "    X.append(img_read(src,file)/255.)  # normalize the image\n",
    "    Y.append(float(file[:-4]))  # extract label from filename\n",
    "\n",
    "# convert the data to array\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# check the shape of the data\n",
    "print('X.shape:',np.shape(X),'Y.shape:',np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "executionInfo": {
     "elapsed": 789,
     "status": "ok",
     "timestamp": 1727862013448,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "kBXB9HPJSizy",
    "outputId": "fb30d144-a95d-4c21-b000-ba2c7ac31280"
   },
   "outputs": [],
   "source": [
    "# display images\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "nplot = 5\n",
    "\n",
    "for i in range(1,nplot+1):\n",
    "    ax = fig.add_subplot(1,nplot,i)\n",
    "    ax.imshow(X[i, :, :],cmap = plt.cm.bone)\n",
    "    ax.set_title(f\"Y {Y[i]:.2f}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1727862090494,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "nbz9a30YHdLc",
    "outputId": "2751e94f-7f16-4c31-daf7-ed31b6f1a7e3"
   },
   "outputs": [],
   "source": [
    "# split the data into 80% training set and 20% test set (test set can be considered as validation)\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(X,Y, test_size=0.2, random_state=1,shuffle=True)\n",
    "train_images  =  2* train_images - 1\n",
    "test_images  =  2* test_images - 1\n",
    "# print the shapes and types of the training and test datasets\n",
    "print(np.shape(train_images), np.shape(test_images))\n",
    "print(type(train_images), type(test_images))\n",
    "\n",
    "print(np.shape(train_labels),np.shape(test_labels))\n",
    "print(type(train_labels), type(test_labels))\n",
    "\n",
    "# reshape the (image,image) images into (image*image,) size vectors\n",
    "#train_images = train_images.reshape((len(train_images), np.prod(train_images.shape[1:]))).astype('float32')\n",
    "#test_images = test_images.reshape((len(test_images), np.prod(test_images.shape[1:]))).astype('float32')\n",
    "\n",
    "\n",
    "print(\"Datasets_train_shape:{}  Datasets_test_shape:{}\".format(np.shape(train_images),np.shape(test_images)))\n",
    "\n",
    "# convert data to PyTorch tensors\n",
    "train_images = torch.tensor(train_images, dtype=torch.float32).to(device)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32) # don't need to use labels\n",
    "test_images = torch.tensor(test_images, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.float32)   # don't need to use labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWGe2_DQJJXm"
   },
   "source": [
    "## 5. Set to model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1727862091712,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "fgVfbYbRm5tv"
   },
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, conv_layers_channels=[16, 32, 64], conv_transpose_layers_channels=None):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        if conv_transpose_layers_channels is None:\n",
    "            conv_transpose_layers_channels = conv_layers_channels[::-1]\n",
    "            conv_transpose_layers_channels[-1] = 1\n",
    "        num_conv_t_layers = len(conv_transpose_layers_channels)\n",
    "        # automatically determine the number of convolutional layers\n",
    "        num_conv_layers = len(conv_layers_channels)\n",
    "\n",
    "        # define the convolutional layers\n",
    "        self.encoder = nn.ModuleList()\n",
    "        in_channels = 1  # input channels (1 for grayscale images)\n",
    "        for i in range(num_conv_layers-1):\n",
    "            out_channels = conv_layers_channels[i]\n",
    "            self.encoder.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1))\n",
    "            self.encoder.append(nn.ReLU())\n",
    "#            self.encoder.append(nn.MaxPool2d(kernel_size=2))\n",
    "            in_channels = out_channels  # set the output channels of this layer as input for the next layer\n",
    "        out_channels = conv_layers_channels[num_conv_layers-1]\n",
    "        self.encoder.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=4, stride=2, padding=1))\n",
    "\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "        in_channels = out_channels  # input channels (1 for grayscale images)\n",
    "        out_channels = conv_transpose_layers_channels[0]\n",
    "        for i in range(num_conv_t_layers-1):\n",
    "            out_channels = conv_transpose_layers_channels[i]\n",
    "            self.decoder.append(nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            self.decoder.append(nn.ReLU())\n",
    "            in_channels = out_channels  # set the output channels of this layer as input for the next layer\n",
    "        out_channels = conv_transpose_layers_channels[num_conv_t_layers-1]\n",
    "        self.decoder.append(nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "\n",
    "#        self.decoder.append(nn.MaxPool2d(kernel_size=32))\n",
    "    def forward(self, x):\n",
    "        # pass the input through the convolutional layers with ReLU and MaxPooling\n",
    "        for conv in self.encoder:\n",
    "            x = conv(x)\n",
    "        for conv in self.decoder:\n",
    "            x = conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUHWoLeYf4Kk"
   },
   "source": [
    "## 6. Set model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1727862093260,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "4K-HjAnym5tv",
    "outputId": "df26d882-349c-4397-ef78-2219ccb1d0f4"
   },
   "outputs": [],
   "source": [
    "conv_layers_channels = [32,64,128]\n",
    "#conv_transpose_layers_channels = [128,64,32]\n",
    "cae = ConvAutoencoder(conv_layers_channels=conv_layers_channels).to(device)\n",
    "\n",
    "summary(cae, (1, 56, 56))  # input shape: (channels, height, width)\n",
    "\n",
    "cae  # print model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1727862095352,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "phgwC5DSm5tv"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001;num_epochs=100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln3cG3Q8gJDL"
   },
   "source": [
    "## 7. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 48296,
     "status": "ok",
     "timestamp": 1727862144858,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "v3gfjB_mIk7N",
    "outputId": "3d03a330-17d6-45f2-c03e-beeecde023f1"
   },
   "outputs": [],
   "source": [
    "# create a DataLoader for the training data\n",
    "train_dataset = TensorDataset(train_images, train_images)  # labels are not needed in autoencoder\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "n = 10\n",
    "# set to loss function & optimizer\n",
    "\n",
    "# define optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(cae.parameters(), lr=learning_rate)\n",
    "average_loss_trains = []\n",
    "# train the VAE model\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    cae.train()\n",
    "    for x, _ in train_loader:\n",
    "        x = x.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        output = cae(x)\n",
    "        loss = criterion(output, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 50 == 0 or epoch == 0:\n",
    "        n = 10  # how many images to display\n",
    "        plt.figure(figsize=(15, 4)) # figure size\n",
    "        for i in range(n):\n",
    "            # display original\n",
    "            ax = plt.subplot(2, n, i + 1)\n",
    "            plt.imshow(x.cpu()[i].reshape(56, 56), vmin=0, vmax=1, cmap=\"gray\")\n",
    "            plt.title(f\"Input{i+1}\")\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "            # display reconstruction\n",
    "            ax = plt.subplot(2, n, i + 1 + n)\n",
    "            plt.imshow(output.detach().cpu()[i].reshape(56, 56), vmin=0, vmax=1, cmap=\"gray\")\n",
    "            plt.title(f\"Recon{i+1}\")\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "        plt.suptitle(f\"Reconstruction at Epoch {epoch+1}\")\n",
    "        plt.show()\n",
    "\n",
    "    average_loss_train = total_loss / len(train_loader)\n",
    "    average_loss_trains.append(average_loss_train)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train_loss: {average_loss_train:.4f}\")\n",
    "\n",
    "# save the trained model\n",
    "torch.save(cae.state_dict(), './model/CAE_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1727862146582,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "e-fo3XB7m5tv",
    "outputId": "b053753a-8dd0-4e40-9709-6a30c22204e6"
   },
   "outputs": [],
   "source": [
    "plt.plot(average_loss_trains)\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"mse\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Pdc3w4ym5tv"
   },
   "source": [
    "## 8. Load & Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmhkaSyGm5tv"
   },
   "source": [
    "- Compare the orginial image and the reconstructed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "executionInfo": {
     "elapsed": 958,
     "status": "ok",
     "timestamp": 1727842656856,
     "user": {
      "displayName": "Smart Design Lab",
      "userId": "14049043496649695794"
     },
     "user_tz": -540
    },
    "id": "uVzk0N7qm5tv",
    "outputId": "ea662b5b-2a29-43f5-c9ea-9104a567a906"
   },
   "outputs": [],
   "source": [
    "cae.eval().to(device)\n",
    "\n",
    "# proceed reconstruction of the bracket images through the trained autoencoder model\n",
    "test_images = test_images#.unsqueeze(1)\n",
    "recon_x_test = cae(test_images)\n",
    "n = 10  # how many images to display\n",
    "\n",
    "plt.figure(figsize=(15, 4)) # figure size\n",
    "\n",
    "for i in range(n):\n",
    "\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(test_images.squeeze().cpu()[i], vmin=0, vmax=1, cmap=\"gray\")  # reshape the images to show\n",
    "    plt.title(\"Input\"+str(i+1))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(recon_x_test.squeeze().detach().cpu()[i],vmin=0, vmax=1, cmap=\"gray\")  # reshape the images to show\n",
    "    plt.title(\"Recon\"+str(i+1))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
